# -*- coding: utf-8 -*-
"""Stock1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMXYRZ8jVSX0eKhAyxtEHp-uj9tQuebE
"""

# ! pip install yfinance numpy pandas matplotlib scikit-learn tensorflow streamlit shap

# import libraries
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import streamlit as st
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.optimizers import RMSprop

# extract data
df = yf.download('AAPL', start='2025-01-20', end='2025-03-10', interval='30m')
# df = yf.download('AAPL', start='2025-01-01', end='2025-03-13', interval='60m')

df.to_csv('apple_data.csv')

df.describe()
print(df.head())
data = pd.DataFrame(df)

# check the number of data
print(data.shape)

"""1. Close: The last price at which a stock is traded
2. Open: The price at which a stock first trades when the market opens
3. High: the higest price reached by stock
4. Low: the lowest price reached by stock
5. Volume: The total number of shared traded in a stock over a given period
6. Returns: The percentage change in the stock price over a given period
7. RSI (Relative Strength Index): A momentum oscillator that measures the speed and change of the price movements to assess overbough or oversold.
8. MACD (Moving Average Convergence Divergence): A trend following momentum indicator that shows the relationship between two moving averages of a stock's price.
9. Upper Band: The upper boundary of the Bollinger Bands ( two standard above moving average)
10. Lower Band: The lower boundary of the bollinger Bands, typically two standard devaitions below the moving average.
11. ATR (Average True Range): A volatility indicator that measures the average range between the high and low prices over a specific period.
"""

# features of data
features = ['Close', 'High', 'Low', 'Open', 'Volume']
data = data[features]

# include price change (returns)
data['Returns'] = data['Close'].pct_change()

# Relative Strength Index (RSI)
delta = data['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
rs = gain / loss
data['RSI'] = 100 - (100 / (1 + rs))

# Moving Average Convergence Divergence (MACD)
exp1 = data['Close'].ewm(span=12, adjust=False).mean()
exp2 = data['Close'].ewm(span=26, adjust=False).mean()
data['MACD'] = exp1 - exp2

# Bollinger Bands
data['Rolling Mean'] = data['Close'].rolling(window=20).mean()
data['Rolling Std'] = data['Close'].rolling(window=20).std()
data['Upper Band'] = data['Rolling Mean'] + (data['Rolling Std'] * 2)
data['Lower Band'] = data['Rolling Mean'] - (data['Rolling Std'] * 2)

# Average True Range (ATR)
high_low = data['High'] - data['Low']
high_close = np.abs(data['High'] - data['Close'].shift())
low_close = np.abs(data['Low'] - data['Close'].shift())
data['TR'] = np.maximum(high_low, np.maximum(high_close, low_close))
data['ATR'] = data['TR'].rolling(window=14).mean()

# drop null values
processed_data = data.dropna()

print("Complete")

"""
Various features were implemented. After implementing the features, the data with null values were removed
Define a scalar to standardize the data
"""

# copy a value of Close to new future close
processed_data['Future Close'] = processed_data['Close']

# Chnage the value of 1st column with 2nd column and continue unitil end. For last data put NaN
for i in range(1, len(processed_data)):
    processed_data.iloc[i, processed_data.columns.get_loc('Future Close')] = processed_data.iloc[i-1, processed_data.columns.get_loc('Close')]

# drop null
processed_data = processed_data.dropna()

# print head of processed_data
print(processed_data.head())

# # correlation of processed _data
# correlation_matrix = processed_data.corr()
# print(correlation_matrix)

correlations = processed_data.corr()["Future Close"].drop("Future Close")
sorted_correlations = correlations.sort_values
print(correlations)

# features
features = ['Close', 'Open', 'High', 'Low', 'Volume', 'Returns', 'RSI', 'MACD', 'Upper Band', 'Lower Band', 'ATR']

# define a scalar
scaler = MinMaxScaler(feature_range=(0, 1))

# scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(processed_data[features])

df_scaled = pd.DataFrame(scaled_data, columns=features, index=processed_data.index)

print(df_scaled.head())

"""Create a sequence to handle data for training for 60 data for training."""

# Create X and y where X have last 60 data and y has future data
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(1 if data[i+seq_length, 0] > data[i+seq_length-1, 0] else 0)
    return np.array(X), np.array(y)

seq_length = 60
X, y = create_sequences(df_scaled.values, seq_length)

# Reshape X to be suitbale for LSTM
X = X.reshape(X.shape[0], X.shape[1], len(features))

# split the data into train and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)

# furthur split train data into validate and train
X_train_real, X_val, y_train_real, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)

# print the basic shape for each dataset
print("Training set shape:", X_train_real.shape)
print("Validation set shape:", X_val.shape)
print("Test set shape:", X_test.shape)

# print some column of training set
print(X_train_real[1][0])
print(X_test.shape)

# create a LSTM model
model = Sequential()
model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(128, activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# optimizer = RMSprop(learning_rate=0.0001, rho=0.9)
optimizer = Adam(learning_rate = 0.0001)
# optimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=True, weight_decay=0.0001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001)
history = model.fit(X_train_real, y_train_real, epochs=20, batch_size=16, validation_data=(X_val, y_val), callbacks=[lr_scheduler])

# calculate the efficieny of the model
val_loss, val_acc = model.evaluate(X_val, y_val)
print("Validation Loss:", val_loss)
print("Validation Accuracy:", val_acc)

# # save the mode
# model.save('lstm_model.h5')
# model.save("lstm_stock_model.keras")

from sklearn.metrics import roc_curve

y_probs = model.predict(X_val)  # Get probabilities
fpr, tpr, thresholds = roc_curve(y_val, y_probs)

# Find the best threshold (closest to top-left of ROC curve)
optimal_idx = (tpr - fpr).argmax()
optimal_threshold = thresholds[optimal_idx]

print("Best Threshold:", optimal_threshold)

# from sklearn.metrics import precision_recall_curve

# precisions, recalls, thresholds = precision_recall_curve(y_val, y_probs)

# # Find the threshold where Precision â‰ˆ Recall
# optimal_idx = (precisions - recalls).argmin()
# optimal_threshold = thresholds[optimal_idx]

# print("Best Threshold:", optimal_threshold)

# # print the test results for 10 validation set
# print(model.predict(X_val[:10]))
# print(y_val[:10])

# # print the model prediction in line on 10 validation set and set 1 if it is greater than .5 esle 0
# y_pred = (model.predict(X_val[:10]) > 0.5).astype(int)
# print(y_pred)

# draw a confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred = (model.predict(X_test) > optimal_threshold).astype(int)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# draw confusion matrix for validation
y_val_pred = (model.predict(X_val) > optimal_threshold).astype(int)
cm = confusion_matrix(y_val, y_val_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Validation Set')
plt.show()

# print classifivccatiopn table
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# draw confusion matrix for train data
y_train_pred = (model.predict(X_train_real) > optimal_threshold).astype(int)
cm = confusion_matrix(y_train_real, y_train_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Train Set')
plt.show()

# draw the graph of loss function
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper right')
